---
title: "class09_markdown"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Save data to directory then read as CSV
```{r}
fna.data <- "data/WisconsinCancer.csv"
wisc.df <- read.csv(fna.data)
```

Convert features of data into matrix becaue there are errors in previous file (zip code, etc)
```{r}
wisc.data <- as.matrix(wisc.df[,3:32] )
```

Set Row Names (re-add patient ID in data set)
```{r}
row.names(wisc.data) <- wisc.df$i
```

Create Vector for checking the diagonosis replace "M" with 1 and none as 0
```{r}
##tells you how many beningn and how many tumors cells
##table(wisc.df$diagnosis)

##checks if M then assign T/F value
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```

Q1. How many observations are in this dataset? --> 569
```{r}
nrow(wisc.data)
```
Q2. How many variables/features in the data are suffixed with _mean? --> 10
```{r}
inds <- grep ("_mean", colnames(wisc.data))
length(inds)
```
Q3. How many of the observations have a malignant diagnosis? --> 212
```{r}
sum(diagnosis)
```

**Section 2 PCA**

Check column mean and stdev --> looks like need to rescale
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

Perform PCA on wisc.data
```{r}
wisc.pr <- prcomp(wisc.data, scale. =T)
##can see PCA results w code below; proportion of varaince shows % that each PC has
##PC1+PC2 gives 62% of variance, if go to PC5 get 85%
##summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)? --> 44.27%
```{r}
summary(wisc.pr)
```
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data? --> 3
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? --> 7

Plot wisc.pr
```{r}
## biplot(wisc.pr)
```
Looks quite ugly and not very useful because too messy

We need to make our own plot
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2], col=diagnosis+1, xlab = "PC1", ylab = "PC2")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? --> the red and black points are closer to each other
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3], col=diagnosis+1, xlab = "PC1", ylab = "PC3")
```

Calculate Variance
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Principal Compoent / Total variance 
```{r}
# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Better way to show data
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Make Cumulative Plot Sum of data variance
```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

**Hierarchical Clustering**

Scale wisc.data
```{r}
data.scaled <- scale(wisc.data)
```

Calculate Euclidean distance b/w pairs of observations
```{r}
data.dist <- dist(data.scaled)
```

Hierarchical Method using hclust()
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters? --> 19
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

Use cutree() to make it so we only have 4 clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

Table to compare cluster membership to actual diagnosis
```{r}
table(wisc.hclust.clusters, diagnosis)
```
create test_clust function????
```{r}
test_clust <- function(cluster_start, cluster_finish) {
  cluster_value <- cluster_start
  while(cluster_value<=cluster_finish){
      wisc.hclust.clusters <- cutree(wisc.hclust, k=cluster_value)
      return(table(wisc.hclust.clusters, diagnosis))
      cluster_value <- cluster_value+1
      ##test_clust(cluster_value, cluster_finish)
  }

}
test_clust(10,10)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10? --> difficult

**4: k means clustering**

```{r}
#wisc.km <- kmeans(, centers= ___, nstart= ___)
```

**5 Combining Methods**
```{r}
#grps <- cutree(wisc.pr.hclust, k=2)
#table(grps)
```

```{r}
##library(rgl)
##plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=diagnosis+1)
```

**Prediction**
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

plot function against data
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis+1)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
```





